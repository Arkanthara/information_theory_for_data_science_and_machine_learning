{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indication: marche pour point a et point b...\n",
    "$$I(X;Y, Z) = I(X; Y) + I(X; Z|Y) = \\dots (Autre\\ possibilité\\ de\\ décomposition...\\ voir\\ cours)$$\n",
    "Considérez la chaîne de Markov suivante :\n",
    "\n",
    "X → Y → Z\n",
    "\n",
    "Prouvez que:\n",
    "\n",
    "1. $I(X;Z) \\leq I(X;Y)$\n",
    "2. $I(X; Y|Z) \\leq I(X;Z)$\n",
    "\n",
    "Voici les preuves que je propose:\n",
    "\n",
    "1. Selon les règles de chaînage, on a que:\n",
    "   \n",
    "   $$I(X;Y, Z) = I(X;Y) + I(X; Z|Y) = I(X; Z) + I(X; Y|Z)$$\n",
    "\n",
    "   Selon le cours, on a que:\n",
    "   $$I(X; Y|Z) = H(X|Z) - H(X|Y, Z)$$\n",
    "   \n",
    "   Donc:\n",
    "   $$I(X; Z|Y) = H(X|Y) - H(X|Z,Y)$$\n",
    "   On a que:\n",
    "   $$H(X|Z, Y) = -\\sum_{x \\in X}\\sum_{y \\in Y}\\sum_{z \\in Z}p(x, y, z)log_2(p(x|y, z))$$\n",
    "   Selon les chaînes de Markov, en regardant le schéma des relations entre nos variables, on peut voir que pour $X$, le fait de connaître $Z$ et $Y$ n'apporte aucune information supplémentaire que le fait de connaître seulement $Y$. Donc $p(x|y, z) = p(x|y)$. On a donc:\n",
    "   $$H(X|Z, Y) = -\\sum_{x \\in X}\\sum_{y \\in Y}\\sum_{z \\in Z}p(x, y, z)log_2(p(x|y, z))$$\n",
    "   $$= -\\sum_{x \\in X}\\sum_{y \\in Y}\\sum_{z \\in Z}p(x, y, z)log_2(p(x|y))$$\n",
    "   $$= -\\sum_{x \\in X}\\sum_{y \\in Y}p(x, y)log_2(p(x|y))$$\n",
    "   $$= H(X|Y)$$\n",
    "   Donc:\n",
    "   $$I(X; Z|Y) = H(X|Y) - H(X|Z,Y) = H(X|Y) - H(X|Y) = 0$$\n",
    "\n",
    "   On a donc que:\n",
    "   $$I(X;Y, Z) = I(X;Y) + I(X; Z|Y) = I(X; Z) + I(X; Y|Z)$$\n",
    "   $$\\Leftrightarrow I(X; Y) + I(X; Z|Y) = I(X; Z) + I(X; Y|Z)$$\n",
    "   $$\\Leftrightarrow I(X; Y) + 0 = I(X; Z) + I(X; Y|Z)$$\n",
    "   Comme l'information mutuelle est toujours positive ou égale a 0, on a que:\n",
    "   $$I(X; Y) = I(X; Z) + I(X; Y|Z)$$\n",
    "   $$\\Leftrightarrow \\boxed{I(X; Y)\\geq I(X; Z)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indice: utiliser comment on écrit l'information mutuelle en entropie + le fait que log_2k est la plus grande entropie pour une variable discrète...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, on ne dit pas que X1 et X2 sont indépendants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autre indice:\n",
    "$$I(X_1,X_2;Y_1,Y_2) = I(X_1, X_2;Y_1) + I(X_1,X_2;Y_2|Y_1)= \\dots=I(X_2; Y_1) + I(X_1; Y_1|Y_2) + I(X_2;Y_2)$$\n",
    "Puis décomposer ces termes en entropie, ce qui permet d'arriver à la formule demandée..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
